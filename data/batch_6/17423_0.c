void vp9_idct8x8_12_add_sse2 ( const int16_t * input , uint8_t * dest , int stride ) {
 const __m128i zero = _mm_setzero_si128 ( ) ;
 const __m128i rounding = _mm_set1_epi32 ( DCT_CONST_ROUNDING ) ;
 const __m128i final_rounding = _mm_set1_epi16 ( 1 << 4 ) ;
 const __m128i stg1_0 = pair_set_epi16 ( cospi_28_64 , - cospi_4_64 ) ;
 const __m128i stg1_1 = pair_set_epi16 ( cospi_4_64 , cospi_28_64 ) ;
 const __m128i stg1_2 = pair_set_epi16 ( - cospi_20_64 , cospi_12_64 ) ;
 const __m128i stg1_3 = pair_set_epi16 ( cospi_12_64 , cospi_20_64 ) ;
 const __m128i stg2_0 = pair_set_epi16 ( cospi_16_64 , cospi_16_64 ) ;
 const __m128i stg2_1 = pair_set_epi16 ( cospi_16_64 , - cospi_16_64 ) ;
 const __m128i stg2_2 = pair_set_epi16 ( cospi_24_64 , - cospi_8_64 ) ;
 const __m128i stg2_3 = pair_set_epi16 ( cospi_8_64 , cospi_24_64 ) ;
 const __m128i stg3_0 = pair_set_epi16 ( - cospi_16_64 , cospi_16_64 ) ;
 __m128i in0 , in1 , in2 , in3 , in4 , in5 , in6 , in7 ;
 __m128i stp1_0 , stp1_1 , stp1_2 , stp1_3 , stp1_4 , stp1_5 , stp1_6 , stp1_7 ;
 __m128i stp2_0 , stp2_1 , stp2_2 , stp2_3 , stp2_4 , stp2_5 , stp2_6 , stp2_7 ;
 __m128i tmp0 , tmp1 , tmp2 , tmp3 , tmp4 , tmp5 , tmp6 , tmp7 ;
 in0 = _mm_load_si128 ( ( const __m128i * ) input ) ;
 in1 = _mm_load_si128 ( ( const __m128i * ) ( input + 8 * 1 ) ) ;
 in2 = _mm_load_si128 ( ( const __m128i * ) ( input + 8 * 2 ) ) ;
 in3 = _mm_load_si128 ( ( const __m128i * ) ( input + 8 * 3 ) ) ;
 TRANSPOSE_8X8_10 ( in0 , in1 , in2 , in3 , in0 , in1 ) ;
 {
 const __m128i lo_17 = _mm_unpackhi_epi16 ( in0 , zero ) ;
 const __m128i lo_35 = _mm_unpackhi_epi16 ( in1 , zero ) ;
 tmp0 = _mm_madd_epi16 ( lo_17 , stg1_0 ) ;
 tmp2 = _mm_madd_epi16 ( lo_17 , stg1_1 ) ;
 tmp4 = _mm_madd_epi16 ( lo_35 , stg1_2 ) ;
 tmp6 = _mm_madd_epi16 ( lo_35 , stg1_3 ) ;
 tmp0 = _mm_add_epi32 ( tmp0 , rounding ) ;
 tmp2 = _mm_add_epi32 ( tmp2 , rounding ) ;
 tmp4 = _mm_add_epi32 ( tmp4 , rounding ) ;
 tmp6 = _mm_add_epi32 ( tmp6 , rounding ) ;
 tmp0 = _mm_srai_epi32 ( tmp0 , DCT_CONST_BITS ) ;
 tmp2 = _mm_srai_epi32 ( tmp2 , DCT_CONST_BITS ) ;
 tmp4 = _mm_srai_epi32 ( tmp4 , DCT_CONST_BITS ) ;
 tmp6 = _mm_srai_epi32 ( tmp6 , DCT_CONST_BITS ) ;
 stp1_4 = _mm_packs_epi32 ( tmp0 , tmp2 ) ;
 stp1_5 = _mm_packs_epi32 ( tmp4 , tmp6 ) ;
 }
 {
 const __m128i lo_04 = _mm_unpacklo_epi16 ( in0 , zero ) ;
 const __m128i lo_26 = _mm_unpacklo_epi16 ( in1 , zero ) ;
 tmp0 = _mm_madd_epi16 ( lo_04 , stg2_0 ) ;
 tmp2 = _mm_madd_epi16 ( lo_04 , stg2_1 ) ;
 tmp4 = _mm_madd_epi16 ( lo_26 , stg2_2 ) ;
 tmp6 = _mm_madd_epi16 ( lo_26 , stg2_3 ) ;
 tmp0 = _mm_add_epi32 ( tmp0 , rounding ) ;
 tmp2 = _mm_add_epi32 ( tmp2 , rounding ) ;
 tmp4 = _mm_add_epi32 ( tmp4 , rounding ) ;
 tmp6 = _mm_add_epi32 ( tmp6 , rounding ) ;
 tmp0 = _mm_srai_epi32 ( tmp0 , DCT_CONST_BITS ) ;
 tmp2 = _mm_srai_epi32 ( tmp2 , DCT_CONST_BITS ) ;
 tmp4 = _mm_srai_epi32 ( tmp4 , DCT_CONST_BITS ) ;
 tmp6 = _mm_srai_epi32 ( tmp6 , DCT_CONST_BITS ) ;
 stp2_0 = _mm_packs_epi32 ( tmp0 , tmp2 ) ;
 stp2_2 = _mm_packs_epi32 ( tmp6 , tmp4 ) ;
 tmp0 = _mm_adds_epi16 ( stp1_4 , stp1_5 ) ;
 tmp1 = _mm_subs_epi16 ( stp1_4 , stp1_5 ) ;
 stp2_4 = tmp0 ;
 stp2_5 = _mm_unpacklo_epi64 ( tmp1 , zero ) ;
 stp2_6 = _mm_unpackhi_epi64 ( tmp1 , zero ) ;
 }
 {
 const __m128i lo_56 = _mm_unpacklo_epi16 ( stp2_5 , stp2_6 ) ;
 tmp4 = _mm_adds_epi16 ( stp2_0 , stp2_2 ) ;
 tmp6 = _mm_subs_epi16 ( stp2_0 , stp2_2 ) ;
 stp1_2 = _mm_unpackhi_epi64 ( tmp6 , tmp4 ) ;
 stp1_3 = _mm_unpacklo_epi64 ( tmp6 , tmp4 ) ;
 tmp0 = _mm_madd_epi16 ( lo_56 , stg3_0 ) ;
 tmp2 = _mm_madd_epi16 ( lo_56 , stg2_0 ) ;
 tmp0 = _mm_add_epi32 ( tmp0 , rounding ) ;
 tmp2 = _mm_add_epi32 ( tmp2 , rounding ) ;
 tmp0 = _mm_srai_epi32 ( tmp0 , DCT_CONST_BITS ) ;
 tmp2 = _mm_srai_epi32 ( tmp2 , DCT_CONST_BITS ) ;
 stp1_5 = _mm_packs_epi32 ( tmp0 , tmp2 ) ;
 }
 tmp0 = _mm_adds_epi16 ( stp1_3 , stp2_4 ) ;
 tmp1 = _mm_adds_epi16 ( stp1_2 , stp1_5 ) ;
 tmp2 = _mm_subs_epi16 ( stp1_3 , stp2_4 ) ;
 tmp3 = _mm_subs_epi16 ( stp1_2 , stp1_5 ) ;
 TRANSPOSE_4X8_10 ( tmp0 , tmp1 , tmp2 , tmp3 , in0 , in1 , in2 , in3 ) IDCT8 ( in0 , in1 , in2 , in3 , zero , zero , zero , zero , in0 , in1 , in2 , in3 , in4 , in5 , in6 , in7 ) ;
 in0 = _mm_adds_epi16 ( in0 , final_rounding ) ;
 in1 = _mm_adds_epi16 ( in1 , final_rounding ) ;
 in2 = _mm_adds_epi16 ( in2 , final_rounding ) ;
 in3 = _mm_adds_epi16 ( in3 , final_rounding ) ;
 in4 = _mm_adds_epi16 ( in4 , final_rounding ) ;
 in5 = _mm_adds_epi16 ( in5 , final_rounding ) ;
 in6 = _mm_adds_epi16 ( in6 , final_rounding ) ;
 in7 = _mm_adds_epi16 ( in7 , final_rounding ) ;
 in0 = _mm_srai_epi16 ( in0 , 5 ) ;
 in1 = _mm_srai_epi16 ( in1 , 5 ) ;
 in2 = _mm_srai_epi16 ( in2 , 5 ) ;
 in3 = _mm_srai_epi16 ( in3 , 5 ) ;
 in4 = _mm_srai_epi16 ( in4 , 5 ) ;
 in5 = _mm_srai_epi16 ( in5 , 5 ) ;
 in6 = _mm_srai_epi16 ( in6 , 5 ) ;
 in7 = _mm_srai_epi16 ( in7 , 5 ) ;
 RECON_AND_STORE ( dest , in0 ) ;
 RECON_AND_STORE ( dest , in1 ) ;
 RECON_AND_STORE ( dest , in2 ) ;
 RECON_AND_STORE ( dest , in3 ) ;
 RECON_AND_STORE ( dest , in4 ) ;
 RECON_AND_STORE ( dest , in5 ) ;
 RECON_AND_STORE ( dest , in6 ) ;
 RECON_AND_STORE ( dest , in7 ) ;
 }