void vp9_lpf_horizontal_8_sse2 ( unsigned char * s , int p , const unsigned char * _blimit , const unsigned char * _limit , const unsigned char * _thresh , int count ) {
 DECLARE_ALIGNED_ARRAY ( 16 , unsigned char , flat_op2 , 16 ) ;
 DECLARE_ALIGNED_ARRAY ( 16 , unsigned char , flat_op1 , 16 ) ;
 DECLARE_ALIGNED_ARRAY ( 16 , unsigned char , flat_op0 , 16 ) ;
 DECLARE_ALIGNED_ARRAY ( 16 , unsigned char , flat_oq2 , 16 ) ;
 DECLARE_ALIGNED_ARRAY ( 16 , unsigned char , flat_oq1 , 16 ) ;
 DECLARE_ALIGNED_ARRAY ( 16 , unsigned char , flat_oq0 , 16 ) ;
 const __m128i zero = _mm_set1_epi16 ( 0 ) ;
 const __m128i blimit = _mm_load_si128 ( ( const __m128i * ) _blimit ) ;
 const __m128i limit = _mm_load_si128 ( ( const __m128i * ) _limit ) ;
 const __m128i thresh = _mm_load_si128 ( ( const __m128i * ) _thresh ) ;
 __m128i mask , hev , flat ;
 __m128i p3 , p2 , p1 , p0 , q0 , q1 , q2 , q3 ;
 __m128i q3p3 , q2p2 , q1p1 , q0p0 , p1q1 , p0q0 ;
 ( void ) count ;
 q3p3 = _mm_unpacklo_epi64 ( _mm_loadl_epi64 ( ( __m128i * ) ( s - 4 * p ) ) , _mm_loadl_epi64 ( ( __m128i * ) ( s + 3 * p ) ) ) ;
 q2p2 = _mm_unpacklo_epi64 ( _mm_loadl_epi64 ( ( __m128i * ) ( s - 3 * p ) ) , _mm_loadl_epi64 ( ( __m128i * ) ( s + 2 * p ) ) ) ;
 q1p1 = _mm_unpacklo_epi64 ( _mm_loadl_epi64 ( ( __m128i * ) ( s - 2 * p ) ) , _mm_loadl_epi64 ( ( __m128i * ) ( s + 1 * p ) ) ) ;
 q0p0 = _mm_unpacklo_epi64 ( _mm_loadl_epi64 ( ( __m128i * ) ( s - 1 * p ) ) , _mm_loadl_epi64 ( ( __m128i * ) ( s - 0 * p ) ) ) ;
 p1q1 = _mm_shuffle_epi32 ( q1p1 , 78 ) ;
 p0q0 = _mm_shuffle_epi32 ( q0p0 , 78 ) ;
 {
 const __m128i one = _mm_set1_epi8 ( 1 ) ;
 const __m128i fe = _mm_set1_epi8 ( 0xfe ) ;
 const __m128i ff = _mm_cmpeq_epi8 ( fe , fe ) ;
 __m128i abs_p1q1 , abs_p0q0 , abs_q1q0 , abs_p1p0 , work ;
 abs_p1p0 = _mm_or_si128 ( _mm_subs_epu8 ( q1p1 , q0p0 ) , _mm_subs_epu8 ( q0p0 , q1p1 ) ) ;
 abs_q1q0 = _mm_srli_si128 ( abs_p1p0 , 8 ) ;
 abs_p0q0 = _mm_or_si128 ( _mm_subs_epu8 ( q0p0 , p0q0 ) , _mm_subs_epu8 ( p0q0 , q0p0 ) ) ;
 abs_p1q1 = _mm_or_si128 ( _mm_subs_epu8 ( q1p1 , p1q1 ) , _mm_subs_epu8 ( p1q1 , q1p1 ) ) ;
 flat = _mm_max_epu8 ( abs_p1p0 , abs_q1q0 ) ;
 hev = _mm_subs_epu8 ( flat , thresh ) ;
 hev = _mm_xor_si128 ( _mm_cmpeq_epi8 ( hev , zero ) , ff ) ;
 abs_p0q0 = _mm_adds_epu8 ( abs_p0q0 , abs_p0q0 ) ;
 abs_p1q1 = _mm_srli_epi16 ( _mm_and_si128 ( abs_p1q1 , fe ) , 1 ) ;
 mask = _mm_subs_epu8 ( _mm_adds_epu8 ( abs_p0q0 , abs_p1q1 ) , blimit ) ;
 mask = _mm_xor_si128 ( _mm_cmpeq_epi8 ( mask , zero ) , ff ) ;
 mask = _mm_max_epu8 ( abs_p1p0 , mask ) ;
 work = _mm_max_epu8 ( _mm_or_si128 ( _mm_subs_epu8 ( q2p2 , q1p1 ) , _mm_subs_epu8 ( q1p1 , q2p2 ) ) , _mm_or_si128 ( _mm_subs_epu8 ( q3p3 , q2p2 ) , _mm_subs_epu8 ( q2p2 , q3p3 ) ) ) ;
 mask = _mm_max_epu8 ( work , mask ) ;
 mask = _mm_max_epu8 ( mask , _mm_srli_si128 ( mask , 8 ) ) ;
 mask = _mm_subs_epu8 ( mask , limit ) ;
 mask = _mm_cmpeq_epi8 ( mask , zero ) ;
 flat = _mm_max_epu8 ( _mm_or_si128 ( _mm_subs_epu8 ( q2p2 , q0p0 ) , _mm_subs_epu8 ( q0p0 , q2p2 ) ) , _mm_or_si128 ( _mm_subs_epu8 ( q3p3 , q0p0 ) , _mm_subs_epu8 ( q0p0 , q3p3 ) ) ) ;
 flat = _mm_max_epu8 ( abs_p1p0 , flat ) ;
 flat = _mm_max_epu8 ( flat , _mm_srli_si128 ( flat , 8 ) ) ;
 flat = _mm_subs_epu8 ( flat , one ) ;
 flat = _mm_cmpeq_epi8 ( flat , zero ) ;
 flat = _mm_and_si128 ( flat , mask ) ;
 }
 {
 const __m128i four = _mm_set1_epi16 ( 4 ) ;
 unsigned char * src = s ;
 {
 __m128i workp_a , workp_b , workp_shft ;
 p3 = _mm_unpacklo_epi8 ( _mm_loadl_epi64 ( ( __m128i * ) ( src - 4 * p ) ) , zero ) ;
 p2 = _mm_unpacklo_epi8 ( _mm_loadl_epi64 ( ( __m128i * ) ( src - 3 * p ) ) , zero ) ;
 p1 = _mm_unpacklo_epi8 ( _mm_loadl_epi64 ( ( __m128i * ) ( src - 2 * p ) ) , zero ) ;
 p0 = _mm_unpacklo_epi8 ( _mm_loadl_epi64 ( ( __m128i * ) ( src - 1 * p ) ) , zero ) ;
 q0 = _mm_unpacklo_epi8 ( _mm_loadl_epi64 ( ( __m128i * ) ( src - 0 * p ) ) , zero ) ;
 q1 = _mm_unpacklo_epi8 ( _mm_loadl_epi64 ( ( __m128i * ) ( src + 1 * p ) ) , zero ) ;
 q2 = _mm_unpacklo_epi8 ( _mm_loadl_epi64 ( ( __m128i * ) ( src + 2 * p ) ) , zero ) ;
 q3 = _mm_unpacklo_epi8 ( _mm_loadl_epi64 ( ( __m128i * ) ( src + 3 * p ) ) , zero ) ;
 workp_a = _mm_add_epi16 ( _mm_add_epi16 ( p3 , p3 ) , _mm_add_epi16 ( p2 , p1 ) ) ;
 workp_a = _mm_add_epi16 ( _mm_add_epi16 ( workp_a , four ) , p0 ) ;
 workp_b = _mm_add_epi16 ( _mm_add_epi16 ( q0 , p2 ) , p3 ) ;
 workp_shft = _mm_srli_epi16 ( _mm_add_epi16 ( workp_a , workp_b ) , 3 ) ;
 _mm_storel_epi64 ( ( __m128i * ) & flat_op2 [ 0 ] , _mm_packus_epi16 ( workp_shft , workp_shft ) ) ;
 workp_b = _mm_add_epi16 ( _mm_add_epi16 ( q0 , q1 ) , p1 ) ;
 workp_shft = _mm_srli_epi16 ( _mm_add_epi16 ( workp_a , workp_b ) , 3 ) ;
 _mm_storel_epi64 ( ( __m128i * ) & flat_op1 [ 0 ] , _mm_packus_epi16 ( workp_shft , workp_shft ) ) ;
 workp_a = _mm_add_epi16 ( _mm_sub_epi16 ( workp_a , p3 ) , q2 ) ;
 workp_b = _mm_add_epi16 ( _mm_sub_epi16 ( workp_b , p1 ) , p0 ) ;
 workp_shft = _mm_srli_epi16 ( _mm_add_epi16 ( workp_a , workp_b ) , 3 ) ;
 _mm_storel_epi64 ( ( __m128i * ) & flat_op0 [ 0 ] , _mm_packus_epi16 ( workp_shft , workp_shft ) ) ;
 workp_a = _mm_add_epi16 ( _mm_sub_epi16 ( workp_a , p3 ) , q3 ) ;
 workp_b = _mm_add_epi16 ( _mm_sub_epi16 ( workp_b , p0 ) , q0 ) ;
 workp_shft = _mm_srli_epi16 ( _mm_add_epi16 ( workp_a , workp_b ) , 3 ) ;
 _mm_storel_epi64 ( ( __m128i * ) & flat_oq0 [ 0 ] , _mm_packus_epi16 ( workp_shft , workp_shft ) ) ;
 workp_a = _mm_add_epi16 ( _mm_sub_epi16 ( workp_a , p2 ) , q3 ) ;
 workp_b = _mm_add_epi16 ( _mm_sub_epi16 ( workp_b , q0 ) , q1 ) ;
 workp_shft = _mm_srli_epi16 ( _mm_add_epi16 ( workp_a , workp_b ) , 3 ) ;
 _mm_storel_epi64 ( ( __m128i * ) & flat_oq1 [ 0 ] , _mm_packus_epi16 ( workp_shft , workp_shft ) ) ;
 workp_a = _mm_add_epi16 ( _mm_sub_epi16 ( workp_a , p1 ) , q3 ) ;
 workp_b = _mm_add_epi16 ( _mm_sub_epi16 ( workp_b , q1 ) , q2 ) ;
 workp_shft = _mm_srli_epi16 ( _mm_add_epi16 ( workp_a , workp_b ) , 3 ) ;
 _mm_storel_epi64 ( ( __m128i * ) & flat_oq2 [ 0 ] , _mm_packus_epi16 ( workp_shft , workp_shft ) ) ;
 }
 }
 {
 const __m128i t4 = _mm_set1_epi8 ( 4 ) ;
 const __m128i t3 = _mm_set1_epi8 ( 3 ) ;
 const __m128i t80 = _mm_set1_epi8 ( 0x80 ) ;
 const __m128i t1 = _mm_set1_epi8 ( 0x1 ) ;
 const __m128i ps1 = _mm_xor_si128 ( _mm_loadl_epi64 ( ( __m128i * ) ( s - 2 * p ) ) , t80 ) ;
 const __m128i ps0 = _mm_xor_si128 ( _mm_loadl_epi64 ( ( __m128i * ) ( s - 1 * p ) ) , t80 ) ;
 const __m128i qs0 = _mm_xor_si128 ( _mm_loadl_epi64 ( ( __m128i * ) ( s + 0 * p ) ) , t80 ) ;
 const __m128i qs1 = _mm_xor_si128 ( _mm_loadl_epi64 ( ( __m128i * ) ( s + 1 * p ) ) , t80 ) ;
 __m128i filt ;
 __m128i work_a ;
 __m128i filter1 , filter2 ;
 filt = _mm_and_si128 ( _mm_subs_epi8 ( ps1 , qs1 ) , hev ) ;
 work_a = _mm_subs_epi8 ( qs0 , ps0 ) ;
 filt = _mm_adds_epi8 ( filt , work_a ) ;
 filt = _mm_adds_epi8 ( filt , work_a ) ;
 filt = _mm_adds_epi8 ( filt , work_a ) ;
 filt = _mm_and_si128 ( filt , mask ) ;
 filter1 = _mm_adds_epi8 ( filt , t4 ) ;
 filter2 = _mm_adds_epi8 ( filt , t3 ) ;
 filter1 = _mm_unpacklo_epi8 ( zero , filter1 ) ;
 filter1 = _mm_srai_epi16 ( filter1 , 11 ) ;
 filter1 = _mm_packs_epi16 ( filter1 , filter1 ) ;
 filter2 = _mm_unpacklo_epi8 ( zero , filter2 ) ;
 filter2 = _mm_srai_epi16 ( filter2 , 11 ) ;
 filter2 = _mm_packs_epi16 ( filter2 , zero ) ;
 filt = _mm_adds_epi8 ( filter1 , t1 ) ;
 filt = _mm_unpacklo_epi8 ( zero , filt ) ;
 filt = _mm_srai_epi16 ( filt , 9 ) ;
 filt = _mm_packs_epi16 ( filt , zero ) ;
 filt = _mm_andnot_si128 ( hev , filt ) ;
 work_a = _mm_xor_si128 ( _mm_subs_epi8 ( qs0 , filter1 ) , t80 ) ;
 q0 = _mm_loadl_epi64 ( ( __m128i * ) flat_oq0 ) ;
 work_a = _mm_andnot_si128 ( flat , work_a ) ;
 q0 = _mm_and_si128 ( flat , q0 ) ;
 q0 = _mm_or_si128 ( work_a , q0 ) ;
 work_a = _mm_xor_si128 ( _mm_subs_epi8 ( qs1 , filt ) , t80 ) ;
 q1 = _mm_loadl_epi64 ( ( __m128i * ) flat_oq1 ) ;
 work_a = _mm_andnot_si128 ( flat , work_a ) ;
 q1 = _mm_and_si128 ( flat , q1 ) ;
 q1 = _mm_or_si128 ( work_a , q1 ) ;
 work_a = _mm_loadu_si128 ( ( __m128i * ) ( s + 2 * p ) ) ;
 q2 = _mm_loadl_epi64 ( ( __m128i * ) flat_oq2 ) ;
 work_a = _mm_andnot_si128 ( flat , work_a ) ;
 q2 = _mm_and_si128 ( flat , q2 ) ;
 q2 = _mm_or_si128 ( work_a , q2 ) ;
 work_a = _mm_xor_si128 ( _mm_adds_epi8 ( ps0 , filter2 ) , t80 ) ;
 p0 = _mm_loadl_epi64 ( ( __m128i * ) flat_op0 ) ;
 work_a = _mm_andnot_si128 ( flat , work_a ) ;
 p0 = _mm_and_si128 ( flat , p0 ) ;
 p0 = _mm_or_si128 ( work_a , p0 ) ;
 work_a = _mm_xor_si128 ( _mm_adds_epi8 ( ps1 , filt ) , t80 ) ;
 p1 = _mm_loadl_epi64 ( ( __m128i * ) flat_op1 ) ;
 work_a = _mm_andnot_si128 ( flat , work_a ) ;
 p1 = _mm_and_si128 ( flat , p1 ) ;
 p1 = _mm_or_si128 ( work_a , p1 ) ;
 work_a = _mm_loadu_si128 ( ( __m128i * ) ( s - 3 * p ) ) ;
 p2 = _mm_loadl_epi64 ( ( __m128i * ) flat_op2 ) ;
 work_a = _mm_andnot_si128 ( flat , work_a ) ;
 p2 = _mm_and_si128 ( flat , p2 ) ;
 p2 = _mm_or_si128 ( work_a , p2 ) ;
 _mm_storel_epi64 ( ( __m128i * ) ( s - 3 * p ) , p2 ) ;
 _mm_storel_epi64 ( ( __m128i * ) ( s - 2 * p ) , p1 ) ;
 _mm_storel_epi64 ( ( __m128i * ) ( s - 1 * p ) , p0 ) ;
 _mm_storel_epi64 ( ( __m128i * ) ( s + 0 * p ) , q0 ) ;
 _mm_storel_epi64 ( ( __m128i * ) ( s + 1 * p ) , q1 ) ;
 _mm_storel_epi64 ( ( __m128i * ) ( s + 2 * p ) , q2 ) ;
 }
 }