unsigned int vp8_variance_halfpixvar16x16_hv_neon ( const unsigned char * src_ptr , int source_stride , const unsigned char * ref_ptr , int recon_stride , unsigned int * sse ) {
 int i ;
 uint8x8_t d0u8 , d1u8 , d2u8 , d3u8 , d4u8 , d5u8 , d6u8 , d7u8 ;
 int16x4_t d0s16 , d1s16 , d2s16 , d3s16 , d10s16 , d11s16 , d12s16 , d13s16 ;
 int16x4_t d18s16 , d19s16 , d20s16 , d21s16 , d22s16 , d23s16 , d24s16 , d25s16 ;
 uint32x2_t d0u32 , d10u32 ;
 int64x1_t d0s64 , d1s64 , d2s64 , d3s64 ;
 uint8x16_t q0u8 , q1u8 , q2u8 , q3u8 , q4u8 , q5u8 , q6u8 , q7u8 , q8u8 , q9u8 ;
 uint16x8_t q0u16 , q1u16 , q5u16 , q6u16 , q9u16 , q10u16 , q11u16 , q12u16 ;
 int32x4_t q13s32 , q14s32 , q15s32 ;
 int64x2_t q0s64 , q1s64 , q5s64 ;
 q13s32 = vdupq_n_s32 ( 0 ) ;
 q14s32 = vdupq_n_s32 ( 0 ) ;
 q15s32 = vdupq_n_s32 ( 0 ) ;
 q0u8 = vld1q_u8 ( src_ptr ) ;
 q1u8 = vld1q_u8 ( src_ptr + 16 ) ;
 src_ptr += source_stride ;
 q1u8 = vextq_u8 ( q0u8 , q1u8 , 1 ) ;
 q0u8 = vrhaddq_u8 ( q0u8 , q1u8 ) ;
 for ( i = 0 ;
 i < 4 ;
 i ++ ) {
 q2u8 = vld1q_u8 ( src_ptr ) ;
 q3u8 = vld1q_u8 ( src_ptr + 16 ) ;
 src_ptr += source_stride ;
 q4u8 = vld1q_u8 ( src_ptr ) ;
 q5u8 = vld1q_u8 ( src_ptr + 16 ) ;
 src_ptr += source_stride ;
 q6u8 = vld1q_u8 ( src_ptr ) ;
 q7u8 = vld1q_u8 ( src_ptr + 16 ) ;
 src_ptr += source_stride ;
 q8u8 = vld1q_u8 ( src_ptr ) ;
 q9u8 = vld1q_u8 ( src_ptr + 16 ) ;
 src_ptr += source_stride ;
 q3u8 = vextq_u8 ( q2u8 , q3u8 , 1 ) ;
 q5u8 = vextq_u8 ( q4u8 , q5u8 , 1 ) ;
 q7u8 = vextq_u8 ( q6u8 , q7u8 , 1 ) ;
 q9u8 = vextq_u8 ( q8u8 , q9u8 , 1 ) ;
 q1u8 = vrhaddq_u8 ( q2u8 , q3u8 ) ;
 q2u8 = vrhaddq_u8 ( q4u8 , q5u8 ) ;
 q3u8 = vrhaddq_u8 ( q6u8 , q7u8 ) ;
 q4u8 = vrhaddq_u8 ( q8u8 , q9u8 ) ;
 q0u8 = vrhaddq_u8 ( q0u8 , q1u8 ) ;
 q1u8 = vrhaddq_u8 ( q1u8 , q2u8 ) ;
 q2u8 = vrhaddq_u8 ( q2u8 , q3u8 ) ;
 q3u8 = vrhaddq_u8 ( q3u8 , q4u8 ) ;
 q5u8 = vld1q_u8 ( ref_ptr ) ;
 ref_ptr += recon_stride ;
 q6u8 = vld1q_u8 ( ref_ptr ) ;
 ref_ptr += recon_stride ;
 q7u8 = vld1q_u8 ( ref_ptr ) ;
 ref_ptr += recon_stride ;
 q8u8 = vld1q_u8 ( ref_ptr ) ;
 ref_ptr += recon_stride ;
 d0u8 = vget_low_u8 ( q0u8 ) ;
 d1u8 = vget_high_u8 ( q0u8 ) ;
 d2u8 = vget_low_u8 ( q1u8 ) ;
 d3u8 = vget_high_u8 ( q1u8 ) ;
 d4u8 = vget_low_u8 ( q2u8 ) ;
 d5u8 = vget_high_u8 ( q2u8 ) ;
 d6u8 = vget_low_u8 ( q3u8 ) ;
 d7u8 = vget_high_u8 ( q3u8 ) ;
 q9u16 = vsubl_u8 ( d0u8 , vget_low_u8 ( q5u8 ) ) ;
 q10u16 = vsubl_u8 ( d1u8 , vget_high_u8 ( q5u8 ) ) ;
 q11u16 = vsubl_u8 ( d2u8 , vget_low_u8 ( q6u8 ) ) ;
 q12u16 = vsubl_u8 ( d3u8 , vget_high_u8 ( q6u8 ) ) ;
 q0u16 = vsubl_u8 ( d4u8 , vget_low_u8 ( q7u8 ) ) ;
 q1u16 = vsubl_u8 ( d5u8 , vget_high_u8 ( q7u8 ) ) ;
 q5u16 = vsubl_u8 ( d6u8 , vget_low_u8 ( q8u8 ) ) ;
 q6u16 = vsubl_u8 ( d7u8 , vget_high_u8 ( q8u8 ) ) ;
 d18s16 = vreinterpret_s16_u16 ( vget_low_u16 ( q9u16 ) ) ;
 d19s16 = vreinterpret_s16_u16 ( vget_high_u16 ( q9u16 ) ) ;
 q13s32 = vpadalq_s16 ( q13s32 , vreinterpretq_s16_u16 ( q9u16 ) ) ;
 q14s32 = vmlal_s16 ( q14s32 , d18s16 , d18s16 ) ;
 q15s32 = vmlal_s16 ( q15s32 , d19s16 , d19s16 ) ;
 d20s16 = vreinterpret_s16_u16 ( vget_low_u16 ( q10u16 ) ) ;
 d21s16 = vreinterpret_s16_u16 ( vget_high_u16 ( q10u16 ) ) ;
 q13s32 = vpadalq_s16 ( q13s32 , vreinterpretq_s16_u16 ( q10u16 ) ) ;
 q14s32 = vmlal_s16 ( q14s32 , d20s16 , d20s16 ) ;
 q15s32 = vmlal_s16 ( q15s32 , d21s16 , d21s16 ) ;
 d22s16 = vreinterpret_s16_u16 ( vget_low_u16 ( q11u16 ) ) ;
 d23s16 = vreinterpret_s16_u16 ( vget_high_u16 ( q11u16 ) ) ;
 q13s32 = vpadalq_s16 ( q13s32 , vreinterpretq_s16_u16 ( q11u16 ) ) ;
 q14s32 = vmlal_s16 ( q14s32 , d22s16 , d22s16 ) ;
 q15s32 = vmlal_s16 ( q15s32 , d23s16 , d23s16 ) ;
 d24s16 = vreinterpret_s16_u16 ( vget_low_u16 ( q12u16 ) ) ;
 d25s16 = vreinterpret_s16_u16 ( vget_high_u16 ( q12u16 ) ) ;
 q13s32 = vpadalq_s16 ( q13s32 , vreinterpretq_s16_u16 ( q12u16 ) ) ;
 q14s32 = vmlal_s16 ( q14s32 , d24s16 , d24s16 ) ;
 q15s32 = vmlal_s16 ( q15s32 , d25s16 , d25s16 ) ;
 d0s16 = vreinterpret_s16_u16 ( vget_low_u16 ( q0u16 ) ) ;
 d1s16 = vreinterpret_s16_u16 ( vget_high_u16 ( q0u16 ) ) ;
 q13s32 = vpadalq_s16 ( q13s32 , vreinterpretq_s16_u16 ( q0u16 ) ) ;
 q14s32 = vmlal_s16 ( q14s32 , d0s16 , d0s16 ) ;
 q15s32 = vmlal_s16 ( q15s32 , d1s16 , d1s16 ) ;
 d2s16 = vreinterpret_s16_u16 ( vget_low_u16 ( q1u16 ) ) ;
 d3s16 = vreinterpret_s16_u16 ( vget_high_u16 ( q1u16 ) ) ;
 q13s32 = vpadalq_s16 ( q13s32 , vreinterpretq_s16_u16 ( q1u16 ) ) ;
 q14s32 = vmlal_s16 ( q14s32 , d2s16 , d2s16 ) ;
 q15s32 = vmlal_s16 ( q15s32 , d3s16 , d3s16 ) ;
 d10s16 = vreinterpret_s16_u16 ( vget_low_u16 ( q5u16 ) ) ;
 d11s16 = vreinterpret_s16_u16 ( vget_high_u16 ( q5u16 ) ) ;
 q13s32 = vpadalq_s16 ( q13s32 , vreinterpretq_s16_u16 ( q5u16 ) ) ;
 q14s32 = vmlal_s16 ( q14s32 , d10s16 , d10s16 ) ;
 q15s32 = vmlal_s16 ( q15s32 , d11s16 , d11s16 ) ;
 d12s16 = vreinterpret_s16_u16 ( vget_low_u16 ( q6u16 ) ) ;
 d13s16 = vreinterpret_s16_u16 ( vget_high_u16 ( q6u16 ) ) ;
 q13s32 = vpadalq_s16 ( q13s32 , vreinterpretq_s16_u16 ( q6u16 ) ) ;
 q14s32 = vmlal_s16 ( q14s32 , d12s16 , d12s16 ) ;
 q15s32 = vmlal_s16 ( q15s32 , d13s16 , d13s16 ) ;
 q0u8 = q4u8 ;
 }
 q15s32 = vaddq_s32 ( q14s32 , q15s32 ) ;
 q0s64 = vpaddlq_s32 ( q13s32 ) ;
 q1s64 = vpaddlq_s32 ( q15s32 ) ;
 d0s64 = vget_low_s64 ( q0s64 ) ;
 d1s64 = vget_high_s64 ( q0s64 ) ;
 d2s64 = vget_low_s64 ( q1s64 ) ;
 d3s64 = vget_high_s64 ( q1s64 ) ;
 d0s64 = vadd_s64 ( d0s64 , d1s64 ) ;
 d1s64 = vadd_s64 ( d2s64 , d3s64 ) ;
 q5s64 = vmull_s32 ( vreinterpret_s32_s64 ( d0s64 ) , vreinterpret_s32_s64 ( d0s64 ) ) ;
 vst1_lane_u32 ( ( uint32_t * ) sse , vreinterpret_u32_s64 ( d1s64 ) , 0 ) ;
 d10u32 = vshr_n_u32 ( vreinterpret_u32_s64 ( vget_low_s64 ( q5s64 ) ) , 8 ) ;
 d0u32 = vsub_u32 ( vreinterpret_u32_s64 ( d1s64 ) , d10u32 ) ;
 return vget_lane_u32 ( d0u32 , 0 ) ;
 }