unsigned int vp8_variance_halfpixvar16x16_h_neon ( const unsigned char * src_ptr , int source_stride , const unsigned char * ref_ptr , int recon_stride , unsigned int * sse ) {
 int i ;
 uint8x8_t d0u8 , d1u8 , d2u8 , d3u8 , d4u8 , d5u8 , d6u8 , d7u8 ;
 int16x4_t d0s16 , d1s16 , d2s16 , d3s16 , d4s16 , d5s16 , d6s16 , d7s16 ;
 int16x4_t d8s16 , d9s16 , d10s16 , d11s16 , d12s16 , d13s16 , d14s16 , d15s16 ;
 uint32x2_t d0u32 , d10u32 ;
 int64x1_t d0s64 , d1s64 , d2s64 , d3s64 ;
 uint8x16_t q0u8 , q1u8 , q2u8 , q3u8 , q4u8 , q5u8 , q6u8 ;
 uint8x16_t q7u8 , q11u8 , q12u8 , q13u8 , q14u8 ;
 uint16x8_t q0u16 , q1u16 , q2u16 , q3u16 , q4u16 , q5u16 , q6u16 , q7u16 ;
 int32x4_t q8s32 , q9s32 , q10s32 ;
 int64x2_t q0s64 , q1s64 , q5s64 ;
 q8s32 = vdupq_n_s32 ( 0 ) ;
 q9s32 = vdupq_n_s32 ( 0 ) ;
 q10s32 = vdupq_n_s32 ( 0 ) ;
 for ( i = 0 ;
 i < 4 ;
 i ++ ) {
 q0u8 = vld1q_u8 ( src_ptr ) ;
 q1u8 = vld1q_u8 ( src_ptr + 16 ) ;
 src_ptr += source_stride ;
 q2u8 = vld1q_u8 ( src_ptr ) ;
 q3u8 = vld1q_u8 ( src_ptr + 16 ) ;
 src_ptr += source_stride ;
 q4u8 = vld1q_u8 ( src_ptr ) ;
 q5u8 = vld1q_u8 ( src_ptr + 16 ) ;
 src_ptr += source_stride ;
 q6u8 = vld1q_u8 ( src_ptr ) ;
 q7u8 = vld1q_u8 ( src_ptr + 16 ) ;
 src_ptr += source_stride ;
 q11u8 = vld1q_u8 ( ref_ptr ) ;
 ref_ptr += recon_stride ;
 q12u8 = vld1q_u8 ( ref_ptr ) ;
 ref_ptr += recon_stride ;
 q13u8 = vld1q_u8 ( ref_ptr ) ;
 ref_ptr += recon_stride ;
 q14u8 = vld1q_u8 ( ref_ptr ) ;
 ref_ptr += recon_stride ;
 q1u8 = vextq_u8 ( q0u8 , q1u8 , 1 ) ;
 q3u8 = vextq_u8 ( q2u8 , q3u8 , 1 ) ;
 q5u8 = vextq_u8 ( q4u8 , q5u8 , 1 ) ;
 q7u8 = vextq_u8 ( q6u8 , q7u8 , 1 ) ;
 q0u8 = vrhaddq_u8 ( q0u8 , q1u8 ) ;
 q1u8 = vrhaddq_u8 ( q2u8 , q3u8 ) ;
 q2u8 = vrhaddq_u8 ( q4u8 , q5u8 ) ;
 q3u8 = vrhaddq_u8 ( q6u8 , q7u8 ) ;
 d0u8 = vget_low_u8 ( q0u8 ) ;
 d1u8 = vget_high_u8 ( q0u8 ) ;
 d2u8 = vget_low_u8 ( q1u8 ) ;
 d3u8 = vget_high_u8 ( q1u8 ) ;
 d4u8 = vget_low_u8 ( q2u8 ) ;
 d5u8 = vget_high_u8 ( q2u8 ) ;
 d6u8 = vget_low_u8 ( q3u8 ) ;
 d7u8 = vget_high_u8 ( q3u8 ) ;
 q4u16 = vsubl_u8 ( d0u8 , vget_low_u8 ( q11u8 ) ) ;
 q5u16 = vsubl_u8 ( d1u8 , vget_high_u8 ( q11u8 ) ) ;
 q6u16 = vsubl_u8 ( d2u8 , vget_low_u8 ( q12u8 ) ) ;
 q7u16 = vsubl_u8 ( d3u8 , vget_high_u8 ( q12u8 ) ) ;
 q0u16 = vsubl_u8 ( d4u8 , vget_low_u8 ( q13u8 ) ) ;
 q1u16 = vsubl_u8 ( d5u8 , vget_high_u8 ( q13u8 ) ) ;
 q2u16 = vsubl_u8 ( d6u8 , vget_low_u8 ( q14u8 ) ) ;
 q3u16 = vsubl_u8 ( d7u8 , vget_high_u8 ( q14u8 ) ) ;
 d8s16 = vreinterpret_s16_u16 ( vget_low_u16 ( q4u16 ) ) ;
 d9s16 = vreinterpret_s16_u16 ( vget_high_u16 ( q4u16 ) ) ;
 q8s32 = vpadalq_s16 ( q8s32 , vreinterpretq_s16_u16 ( q4u16 ) ) ;
 q9s32 = vmlal_s16 ( q9s32 , d8s16 , d8s16 ) ;
 q10s32 = vmlal_s16 ( q10s32 , d9s16 , d9s16 ) ;
 d10s16 = vreinterpret_s16_u16 ( vget_low_u16 ( q5u16 ) ) ;
 d11s16 = vreinterpret_s16_u16 ( vget_high_u16 ( q5u16 ) ) ;
 q8s32 = vpadalq_s16 ( q8s32 , vreinterpretq_s16_u16 ( q5u16 ) ) ;
 q9s32 = vmlal_s16 ( q9s32 , d10s16 , d10s16 ) ;
 q10s32 = vmlal_s16 ( q10s32 , d11s16 , d11s16 ) ;
 d12s16 = vreinterpret_s16_u16 ( vget_low_u16 ( q6u16 ) ) ;
 d13s16 = vreinterpret_s16_u16 ( vget_high_u16 ( q6u16 ) ) ;
 q8s32 = vpadalq_s16 ( q8s32 , vreinterpretq_s16_u16 ( q6u16 ) ) ;
 q9s32 = vmlal_s16 ( q9s32 , d12s16 , d12s16 ) ;
 q10s32 = vmlal_s16 ( q10s32 , d13s16 , d13s16 ) ;
 d14s16 = vreinterpret_s16_u16 ( vget_low_u16 ( q7u16 ) ) ;
 d15s16 = vreinterpret_s16_u16 ( vget_high_u16 ( q7u16 ) ) ;
 q8s32 = vpadalq_s16 ( q8s32 , vreinterpretq_s16_u16 ( q7u16 ) ) ;
 q9s32 = vmlal_s16 ( q9s32 , d14s16 , d14s16 ) ;
 q10s32 = vmlal_s16 ( q10s32 , d15s16 , d15s16 ) ;
 d0s16 = vreinterpret_s16_u16 ( vget_low_u16 ( q0u16 ) ) ;
 d1s16 = vreinterpret_s16_u16 ( vget_high_u16 ( q0u16 ) ) ;
 q8s32 = vpadalq_s16 ( q8s32 , vreinterpretq_s16_u16 ( q0u16 ) ) ;
 q9s32 = vmlal_s16 ( q9s32 , d0s16 , d0s16 ) ;
 q10s32 = vmlal_s16 ( q10s32 , d1s16 , d1s16 ) ;
 d2s16 = vreinterpret_s16_u16 ( vget_low_u16 ( q1u16 ) ) ;
 d3s16 = vreinterpret_s16_u16 ( vget_high_u16 ( q1u16 ) ) ;
 q8s32 = vpadalq_s16 ( q8s32 , vreinterpretq_s16_u16 ( q1u16 ) ) ;
 q9s32 = vmlal_s16 ( q9s32 , d2s16 , d2s16 ) ;
 q10s32 = vmlal_s16 ( q10s32 , d3s16 , d3s16 ) ;
 d4s16 = vreinterpret_s16_u16 ( vget_low_u16 ( q2u16 ) ) ;
 d5s16 = vreinterpret_s16_u16 ( vget_high_u16 ( q2u16 ) ) ;
 q8s32 = vpadalq_s16 ( q8s32 , vreinterpretq_s16_u16 ( q2u16 ) ) ;
 q9s32 = vmlal_s16 ( q9s32 , d4s16 , d4s16 ) ;
 q10s32 = vmlal_s16 ( q10s32 , d5s16 , d5s16 ) ;
 d6s16 = vreinterpret_s16_u16 ( vget_low_u16 ( q3u16 ) ) ;
 d7s16 = vreinterpret_s16_u16 ( vget_high_u16 ( q3u16 ) ) ;
 q8s32 = vpadalq_s16 ( q8s32 , vreinterpretq_s16_u16 ( q3u16 ) ) ;
 q9s32 = vmlal_s16 ( q9s32 , d6s16 , d6s16 ) ;
 q10s32 = vmlal_s16 ( q10s32 , d7s16 , d7s16 ) ;
 }
 q10s32 = vaddq_s32 ( q10s32 , q9s32 ) ;
 q0s64 = vpaddlq_s32 ( q8s32 ) ;
 q1s64 = vpaddlq_s32 ( q10s32 ) ;
 d0s64 = vget_low_s64 ( q0s64 ) ;
 d1s64 = vget_high_s64 ( q0s64 ) ;
 d2s64 = vget_low_s64 ( q1s64 ) ;
 d3s64 = vget_high_s64 ( q1s64 ) ;
 d0s64 = vadd_s64 ( d0s64 , d1s64 ) ;
 d1s64 = vadd_s64 ( d2s64 , d3s64 ) ;
 q5s64 = vmull_s32 ( vreinterpret_s32_s64 ( d0s64 ) , vreinterpret_s32_s64 ( d0s64 ) ) ;
 vst1_lane_u32 ( ( uint32_t * ) sse , vreinterpret_u32_s64 ( d1s64 ) , 0 ) ;
 d10u32 = vshr_n_u32 ( vreinterpret_u32_s64 ( vget_low_s64 ( q5s64 ) ) , 8 ) ;
 d0u32 = vsub_u32 ( vreinterpret_u32_s64 ( d1s64 ) , d10u32 ) ;
 return vget_lane_u32 ( d0u32 , 0 ) ;
 }